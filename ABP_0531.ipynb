{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "CUDA_DEVICE = 0\n",
    "if CUDA: device='cuda'\n",
    "else: device='cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(Tensor):\n",
    "    return torch.tanh(Tensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RitzBlock(nn.Module):\n",
    "    def __init__(self, n_hid):\n",
    "        super(RitzBlock, self).__init__()\n",
    "        self.n_hid = n_hid\n",
    "        ##Layer 1\n",
    "        self.Weight_layer1 = nn.Parameter(torch.rand(size=(n_hid,n_hid),requires_grad=True)).to(device)\n",
    "        self.Bias_layer1 = nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
    "        ##Layer 2\n",
    "        self.Weight_layer2 = nn.Parameter(torch.rand(size=(n_hid,n_hid),requires_grad=True)).to(device)\n",
    "        self.Bias_layer2 = nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
    "\n",
    "    #Activation function\n",
    "    def layer1(self,Tensor): return activation(torch.matmul(Tensor,self.Weight_layer1)+self.Bias_layer1)\n",
    "    def layer2(self,Tensor): return activation(torch.matmul(Tensor,self.Weight_layer2)+self.Bias_layer2)\n",
    "    \n",
    "    #Forward process\n",
    "    def forward(self,Tensor):\n",
    "        h1=self.layer1(Tensor).to(device)\n",
    "        h2=self.layer2(h1).to(device)\n",
    "        return h2+Tensor #return.shape = (batch_size,tlength+1,nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreModel(nn.Module):\n",
    "    def __init__(self,n_hid):\n",
    "        super(PreModel,self).__init__()\n",
    "        self.n_hid=n_hid\n",
    "        self.pre_Weight=nn.Parameter(torch.rand(size=(1+nptl,n_hid),requires_grad=True)).to(device)\n",
    "        self.pre_Bias=nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
    "        #dof : 자유도     \n",
    "    def forward(self,Tensor):\n",
    "        if self.n_hid>nptl+1: \n",
    "            pad=torch.nn.ZeroPad2d((0,n_hid-1-nptl,0,0))\n",
    "            output=pad(Tensor)\n",
    "        else:\n",
    "            output=activation(torch.matmul(Tensor,self.pre_Weight)+self.pre_Bias)\n",
    "        return output.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworks(nn.Module):\n",
    "    def __init__(self,n_block,n_hid):\n",
    "        super(NeuralNetworks, self).__init__()\n",
    "        self.n_blocks=n_block\n",
    "        self.n_hid=n_hid\n",
    "        \n",
    "        self.nn_Weight=nn.Parameter(torch.rand(size=(n_hid,nptl),requires_grad=True)).to(device)\n",
    "        self.nn_Bias=nn.Parameter(torch.rand(size=(1,nptl),requires_grad=True)).to(device)        \n",
    "        \n",
    "        self.premodel=PreModel(self.n_hid)\n",
    "        \n",
    "        self.Ritzblocks=nn.ModuleList()\n",
    "        for _ in range(self.n_blocks): self.Ritzblocks.append(RitzBlock(self.n_hid))\n",
    "    \n",
    "    def Observable(self,Tensor):\n",
    "        position0=Tensor[...,:-1,...]\n",
    "        batch_size=Tensor.shape[0]\n",
    "        \n",
    "        \n",
    "        return torch.mean(Obs)\n",
    "\n",
    "    def Girsanov_weight(self,force):        \n",
    "        #force.shape=(ntraj,tlength+1,4*nptl)\n",
    "        F_r=force[...,:-1,:2*nptl]\n",
    "        F_v=force[...,:-1,2*nptl:]\n",
    "        iteration=force.shape[0]\n",
    "        position_part=dt*torch.bmm(F_r,torch.permute(F_r,(0,2,1)))\n",
    "        velocity_part=dt*torch.bmm(F_v,torch.permute(F_v,(0,2,1)))\n",
    "        Girsanov=torch.zeros(iteration)\n",
    "        for n in range(iteration):\n",
    "            Girsanov[n]=torch.trace(position_part[n,...]/DT+velocity_part[n,...]/DR)\n",
    "        return -0.5*torch.mean(Girsanov)\n",
    "\n",
    "    #def loss(self,Tensor,noise,force):\n",
    "    #    L=Lambda*(dt*tlength)*self.Observable(Tensor,noise)+self.Girsanov_weight(force)\n",
    "    #    return L\n",
    "\n",
    "    def forward(self,Tensor):\n",
    "        T=self.premodel(Tensor).to(device)\n",
    "        for model in self.Ritzblocks:\n",
    "            val=model(T); T=val\n",
    "            print()\n",
    "        del_u=torch.matmul(T,self.nn_Weight)+self.nn_Bias\n",
    "        return del_u #variational force"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1315e6714f2518a6216a6eec3b047587d10875bf19b853b35d3e5c84c569e2a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
