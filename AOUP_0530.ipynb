{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YWIYvQlQNq1"
      },
      "source": [
        "# I. Building generic model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raqlNxFI22jm"
      },
      "source": [
        "## 0. Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAiZcnQL8xYV",
        "outputId": "bb0f44d8-a21b-4bd1-c496-3a222d07454c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "import numpy as np\n",
        "from tqdm import trange\n",
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "wewt7aMi40Ye",
        "outputId": "185bdd2f-a7b7-40af-e98a-241b4f42a922"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "CUDA_DEVICE = 0\n",
        "if CUDA: device='cuda'\n",
        "else: device='cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBQe1MhQ20fp"
      },
      "source": [
        "## 1. Neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQOfsbVtU1x2"
      },
      "source": [
        "### 1-0 Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zRlaMtfRU1x3"
      },
      "outputs": [],
      "source": [
        "def activation(Tensor): \n",
        "    return torch.tanh(Tensor)\n",
        "\n",
        "def dU(k,position): \n",
        "    return k*position\n",
        "\n",
        "def time_evolution(ntraj,tlength,dt,nptl,gamma,gammaR,DT,DR,k,r,v,force):\n",
        "    State_tensor=torch.zeros(size=(ntraj,tlength+1,4*nptl)); State_list=[]; \n",
        "    Deformed_State_tensor=torch.zeros(size=(ntraj,tlength+1,1+4*nptl)); Deformed_State_list=[]\n",
        "    Noise_tensor=torch.normal(mean=0,std=1/np.sqrt(dt),size=(ntraj,tlength+1,4*nptl)); Noise_list=[]\n",
        "    for traj in range(ntraj):\n",
        "        r_t=r[traj,-1,:]; v_t=v[traj,-1,:]\n",
        "        State_tensor[traj,0,:2*nptl]=r_t; State_tensor[traj,0,2*nptl:]=v_t\n",
        "        Deformed_State_tensor[traj,0,0]=0; Deformed_State_tensor[traj,0,1:]=State_tensor[traj,0,:]\n",
        "        for t in range(tlength):\n",
        "            r_t += dt*(v_t-1/gamma*dU(k,r_t)+force[traj,t,:2*nptl]+np.sqrt(2*DT)*Noise_tensor[traj,t,:2*nptl])\n",
        "            v_t += dt*(-1/gammaR*v_t+force[traj,t,2*nptl:]+np.sqrt(2*DR)*Noise_tensor[traj,t,2*nptl:])\n",
        "            State_tensor[traj,t+1,:2*nptl]=r_t; State_tensor[traj,t+1,2*nptl:]=v_t\n",
        "            Deformed_State_tensor[traj,t+1,0]=(t+1)*dt; Deformed_State_tensor[traj,t+1,1:]=State_tensor[traj,t+1,:]    \n",
        "        State_list.append(State_tensor[traj,...].numpy())\n",
        "        Deformed_State_list.append(Deformed_State_tensor[traj].numpy())\n",
        "        Noise_list.append(Noise_tensor[traj,...].numpy())\n",
        "    return State_tensor, torch.tensor(State_list), torch.tensor(Deformed_State_list), torch.tensor(Noise_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07dNtBSj3lou",
        "tags": []
      },
      "source": [
        "### 1-1. Ritz Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "usMMSPzw3V17"
      },
      "outputs": [],
      "source": [
        "class RitzBlock(nn.Module):\n",
        "    def __init__(self, n_hid):\n",
        "        super(RitzBlock, self).__init__()\n",
        "        self.n_hid = n_hid\n",
        "        ##Layer 1\n",
        "        self.Weight_layer1 = nn.Parameter(torch.rand(size=(n_hid,n_hid),requires_grad=True)).to(device)\n",
        "        self.Bias_layer1 = nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
        "        ##Layer 2\n",
        "        self.Weight_layer2 = nn.Parameter(torch.rand(size=(n_hid,n_hid),requires_grad=True)).to(device)\n",
        "        self.Bias_layer2 = nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
        "\n",
        "    #Activation function\n",
        "    def layer1(self,Tensor): return activation(torch.matmul(Tensor,self.Weight_layer1)+self.Bias_layer1)\n",
        "    def layer2(self,Tensor): return activation(torch.matmul(Tensor,self.Weight_layer2)+self.Bias_layer2)\n",
        "    \n",
        "    #Forward process\n",
        "    def forward(self,Tensor):\n",
        "        h1=self.layer1(Tensor).to(device)\n",
        "        h2=self.layer2(h1).to(device)\n",
        "        return h2+Tensor #return.shape = (tlength+1,nhid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PreModel(nn.Module):\n",
        "    def __init__(self,n_hid):\n",
        "        super(PreModel,self).__init__()\n",
        "        self.n_hid=n_hid\n",
        "        ##premodel parameters\n",
        "        self.pre_Weight=nn.Parameter(torch.rand(size=(4*nptl+1,n_hid),requires_grad=True)).to(device)\n",
        "        self.pre_Bias=nn.Parameter(torch.rand(size=(1,n_hid),requires_grad=True)).to(device)\n",
        "    \n",
        "    def forward(self,Tensor):\n",
        "        if n_hid>4*nptl+1: \n",
        "            pad=torch.nn.ZeroPad2d((0,n_hid-4*nptl-1,0,0))\n",
        "            output=pad(Tensor)\n",
        "        else:\n",
        "            output=activation(torch.matmul(Tensor,self.pre_Weight)+self.pre_Bias)\n",
        "        return output.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PgMAjvAvLHX"
      },
      "source": [
        "### 1-2. Neural networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "oEIr7kn7p2IL"
      },
      "outputs": [],
      "source": [
        "class NeuralNetworks(nn.Module):\n",
        "    def __init__(self,n_block,n_hid):\n",
        "        super(NeuralNetworks, self).__init__()\n",
        "        self.n_blocks=n_block\n",
        "        self.n_hid=n_hid\n",
        "        self.nn_Weight=nn.Parameter(torch.rand(size=(n_hid,4*nptl),requires_grad=True)).to(device)\n",
        "        self.nn_Bias=nn.Parameter(torch.rand(size=(1,4*nptl),requires_grad=True)).to(device)        \n",
        "        self.premodel=PreModel(self.n_hid)\n",
        "        self.Ritzblocks=nn.ModuleList()\n",
        "        for _ in range(self.n_blocks): self.Ritzblocks.append(RitzBlock(self.n_hid))\n",
        "    \n",
        "    def Observable(self,Tensor,noise):\n",
        "        Noise=dt*noise[...,:-1,:2*nptl]; position=Tensor[...,:-1,:2*nptl]\n",
        "        velocity=Tensor[...,:-1,2*nptl:]; velocity1=Tensor[...,1:,2*nptl:]\n",
        "        iteration=Noise.shape[0] #batch size\n",
        "        first=dt*torch.bmm(velocity,torch.permute(velocity,(0,2,1)))\n",
        "        second=dt*torch.bmm(velocity,torch.permute(dU(k,position),(0,2,1)))\n",
        "        third=torch.bmm(velocity+velocity1,torch.permute(Noise,(0,2,1)))/2        \n",
        "        Obs=torch.zeros(iteration) #zero matrix\n",
        "        for n in range(iteration):\n",
        "            Obs[n]=torch.trace(gamma*(first[n,...]-second[n,...]+np.sqrt(2*DT)*third[n,...])/(dt*tlength))\n",
        "        return torch.mean(Obs)\n",
        "\n",
        "    def Girsanov_weight(self,force):        \n",
        "        #force.shape=(ntraj,tlength+1,4*nptl)\n",
        "        F_r=force[...,:-1,:2*nptl]\n",
        "        F_v=force[...,:-1,2*nptl:]\n",
        "        iteration=force.shape[0]\n",
        "        position_part=dt*torch.bmm(F_r,torch.permute(F_r,(0,2,1)))\n",
        "        velocity_part=dt*torch.bmm(F_v,torch.permute(F_v,(0,2,1)))\n",
        "        Girsanov=torch.zeros(iteration)\n",
        "        for n in range(iteration):\n",
        "            Girsanov[n]=torch.trace(position_part[n,...]/DT+velocity_part[n,...]/DR)\n",
        "        return -0.5*torch.mean(Girsanov)\n",
        "\n",
        "    #def loss(self,Tensor,noise,force):\n",
        "    #    L=Lambda*(dt*tlength)*self.Observable(Tensor,noise)+self.Girsanov_weight(force)\n",
        "    #    return L\n",
        "\n",
        "    def forward(self,Tensor):\n",
        "        T=self.premodel(Tensor).to(device)\n",
        "        for model in self.Ritzblocks:\n",
        "            val=model(T); T=val\n",
        "        del_u=torch.matmul(T,self.nn_Weight)+self.nn_Bias\n",
        "        return del_u #variational force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dal7TcaGl7mX"
      },
      "source": [
        "### 1-3 Batch loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HQqWPm4Cl7RN"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset): \n",
        "    def __init__(self, dataset):\n",
        "        data_x = dataset\n",
        "        self.x_data = data_x\n",
        "#         self.y_data = data_y\n",
        "\n",
        "    # 총 데이터의 개수를 리턴\n",
        "    def __len__(self): \n",
        "        return len(self.x_data)\n",
        "    \n",
        "    # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\n",
        "    def __getitem__(self, idx): \n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "#         y = torch.FloatTensor([self.y_data[idx]])\n",
        "        return x\n",
        "\n",
        "def data_to_loader(fullconfigs):\n",
        "    fulldata=CustomDataset(fullconfigs)\n",
        "    full_dataset = fulldata\n",
        "    full_loader = torch.utils.data.DataLoader(full_dataset,batch_size,shuffle=False)\n",
        "    return full_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uPrzg9vCOSn"
      },
      "source": [
        "## 2. Solving SDE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "76BvgDlNPiJv",
        "outputId": "a783fda6-5eca-4986-f45b-eaab823b3c37"
      },
      "outputs": [],
      "source": [
        "def AOUP(Lambda,lr,epochs,k,n_hid,n_block,ntraj,tlength,dt,nptl,gamma,gammaR,DT,DR):\n",
        "    NN=NeuralNetworks(n_block,n_hid)\n",
        "    train_op=optim.SGD(NN.parameters(),lr,momentum=0.9)\n",
        "    NN.train()\n",
        "    train_loss_list=[]; \n",
        "    ActiveWork_list=[]; GirsanovWeight_list=[]\n",
        "    \n",
        "    #Initial condition\n",
        "    r_0=torch.zeros(size=(ntraj,tlength+1,2*nptl)); v_0=torch.zeros_like(r_0)\n",
        "    force=torch.zeros(size=(ntraj,tlength+1,4*nptl))\n",
        "    \n",
        "    for _ in trange(epochs,desc=\"Epoch\"):\n",
        "        train_loss_epoch=[]; ActiveWork_epoch=[]; GirsanovWeight_epoch=[]\n",
        "        # 0. Generate trajectory using previously defined initial condition\n",
        "        States,train,train_real,train_noise=time_evolution(ntraj,tlength,dt,nptl,gamma,gammaR,DT,DR,k,r_0,v_0,force)\n",
        "        #train_real is list of data.shape=(tlength+1,1+4*nptl) <- 진짜 훈련에 사용됨 (loss는 다른 데이터로 계산)\n",
        "        # 1. Update initial state\n",
        "        r_0=States[...,:2*nptl];v_0=States[...,2*nptl:]\n",
        "        #train data를 GPU에 올린다.        \n",
        "        train_loader=data_to_loader(train); train_real_loader=data_to_loader(train_real); noise_loader=data_to_loader(train_noise)\n",
        "        train_n_noise=list(zip(train_loader,train_real_loader,noise_loader))\n",
        "\n",
        "        for _, (data_n_noise) in enumerate(train_n_noise):\n",
        "            data=data_n_noise[0].to(device); data_input=data_n_noise[1].to(device); noise=data_n_noise[2].to(device)\n",
        "            del_u=NN.forward(data_input)\n",
        "            ActiveWork=NN.Observable(data,noise); GirsanovWeight=NN.Girsanov_weight(del_u)\n",
        "            ActiveWork_epoch.append(ActiveWork.detach()); GirsanovWeight_epoch.append(GirsanovWeight.detach())\n",
        "            train_loss=Lambda*dt*tlength*ActiveWork+GirsanovWeight\n",
        "            train_loss_epoch.append(train_loss.item())\n",
        "            train_op.zero_grad()\n",
        "            train_loss.backward()\n",
        "            train_op.step()\n",
        "        \n",
        "        train_loss_list.append(np.mean(train_loss_epoch));\n",
        "        ActiveWork_list.append(np.mean(ActiveWork_epoch));\n",
        "        GirsanovWeight_list.append(np.mean(GirsanovWeight_epoch))\n",
        "    NN=NN.cpu()\n",
        "    return train_loss_list, ActiveWork_list, GirsanovWeight_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6JMqUitPqn6w"
      },
      "outputs": [],
      "source": [
        "Lambda=-2.0\n",
        "learning_rate=0.05\n",
        "epochs=500\n",
        "k=1\n",
        "n_hid=1000; n_block=3\n",
        "ntraj=50; tlength=1000; dt=0.001; nptl=1\n",
        "gamma=1; gammaR=1\n",
        "DT=1; DR=1\n",
        "batch_size=10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "id": "KGFHIY2c3xAq",
        "outputId": "cbb08063-9e0e-4462-8b31-da76c145a7f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:  78%|███████▊  | 388/500 [1:10:52<20:27, 10.96s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/hojun/Desktop/canoneqal/연구/Rare_trajectory/AOUP_0530.ipynb Cell 18'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000017?line=0'>1</a>\u001b[0m loss,AW,GW\u001b[39m=\u001b[39mAOUP(Lambda,learning_rate,epochs,k,n_hid,n_block,ntraj,tlength,dt,nptl,gamma,gammaR,DT,DR)\n",
            "\u001b[1;32m/Users/hojun/Desktop/canoneqal/연구/Rare_trajectory/AOUP_0530.ipynb Cell 16'\u001b[0m in \u001b[0;36mAOUP\u001b[0;34m(Lambda, lr, epochs, k, n_hid, n_block, ntraj, tlength, dt, nptl, gamma, gammaR, DT, DR)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000015?line=28'>29</a>\u001b[0m     train_loss_epoch\u001b[39m.\u001b[39mappend(train_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000015?line=29'>30</a>\u001b[0m     train_op\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000015?line=30'>31</a>\u001b[0m     train_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000015?line=31'>32</a>\u001b[0m     train_op\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojun/Desktop/canoneqal/%EC%97%B0%EA%B5%AC/Rare_trajectory/AOUP_0530.ipynb#ch0000015?line=33'>34</a>\u001b[0m train_loss_list\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(train_loss_epoch));\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
            "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///opt/homebrew/Caskroom/miniforge/base/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss,AW,GW=AOUP(Lambda,learning_rate,epochs,k,n_hid,n_block,ntraj,tlength,dt,nptl,gamma,gammaR,DT,DR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "AOUP_0526.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e1315e6714f2518a6216a6eec3b047587d10875bf19b853b35d3e5c84c569e2a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
